{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pySCENIC workflow\n",
    "\n",
    "The present study is based on the 10X scRNA-seq dataset published by the Allen Institute for Brain Science and publicly available at: https://portal.brain-map.org/atlases-and-data/RNA-seq/mouse-whole-cortex-and-hippocampus-10x. The data was then clustered, and cluster names were assigned based on the Allen Institute proposal for cell type nomenclature (https://portal.brain-map.org/explore/classes/nomenclature). The topology of the taxonomy allowed to define the sex of the mouse from which the cells were isolated, the regions of interest, cell classes (glutamatergic, GABAergic or Non-Neuronal) and subclasses. This information was stored in the metadata table. The metadata was used to subset cells of the hippocampus region from the gene expression matrix. We selected for 13 subclasses of hippocampal cells. The hippocampus gene count matrix was pre-processed in R v3.6.1 according to the Seurat v3.1.5 standard pre-processing workflow for quality control, normalization, and analysis of scRNA-seq data (cf. 10XHip2021_Pre.Processing) and transformed in a loom object for further analysis with pySCENIC.\n",
    "\n",
    "# Description\n",
    "\n",
    "Here we describe the pySCENIC workflow as applied to the loom object.\n",
    "\n",
    "# pySCENIC\n",
    "\n",
    "For a detailed description of the pySCENIC workflow please see here: https://scenic.aertslab.org/\n",
    "\n",
    "# Data availability\n",
    "\n",
    "cf. README to download pySCENIC input '10XHip2021.loom'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data, required packages and request CPU/schedule job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    from pathlib import Path\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--file\",default=None)\n",
    "    parser.add_argument(\"--hpc\", default=False)\n",
    "    parser.add_argument(\"--cpu\", default=1)\n",
    "    parser.add_argument(\"--threads\", default=1)\n",
    "    filename = parser.parse_args().file\n",
    "    HPC = parser.parse_args().hpc\n",
    "    CPU = int(parser.parse_args().cpu)\n",
    "    Threads = int(parser.parse_args().threads)\n",
    "    if filename is None:\n",
    "        filename = input(\"Datafile name.csv: \")\n",
    "    while filename.find(\".loom\") == -1:\n",
    "        filename = input(\"File must be .loom, please try again: \")\n",
    "    \n",
    "    import anndata as ad\n",
    "    import pandas as pd\n",
    "    import loompy as lp\n",
    "    import numpy as np\n",
    "    import bokeh\n",
    "    import os, glob\n",
    "    import pickle\n",
    "    \n",
    "    from distributed import LocalCluster,Client\n",
    "    \n",
    "    from arboreto.utils import load_tf_names\n",
    "    from arboreto.algo import grnboost2\n",
    "    \n",
    "    from pyscenic.rnkdb import FeatherRankingDatabase as RankingDatabase\n",
    "    from pyscenic.utils import modules_from_adjacencies\n",
    "    from pyscenic.prune import prune2df, df2regulons\n",
    "    from pyscenic.aucell import aucell\n",
    "    \n",
    "    import seaborn as sns\n",
    "    \n",
    "    if HPC == False:\n",
    "        SCENICDB_FOLDER=\"/path1/pyScenic/\" \n",
    "        # This folder contains Databases (mm10, mm9 refseq files) and Resources (TF and motifs files)\n",
    "        INPUT_FOLDER=\"/path1/1-Input_Scenic\" \n",
    "        # This folder contains your input loom object\n",
    "        OUTPUT_FOLDER=\"/path1/2-Output_Scenic\"\n",
    "        # This folder contains your results/output files\n",
    "    else: \n",
    "        SCENICDB_FOLDER=\"/path2/pyScenic/\"\n",
    "        INPUT_FOLDER=\"/path2/1-Input_Scenic\"\n",
    "        OUTPUT_FOLDER=\"/path2/2-Output_Scenic\"\n",
    "\n",
    "    \n",
    "    DATA_FOLDER=os.path.join(OUTPUT_FOLDER,(filename.replace(\".loom\",\"\")+\"_Results\"))\n",
    "        \n",
    "    RESOURCES_FOLDER = os.path.join(SCENICDB_FOLDER, \"Resources\")\n",
    "    DATABASE_FOLDER = os.path.join(SCENICDB_FOLDER, \"Databases\")\n",
    "    \n",
    "    if HPC == False:    \n",
    "        SCHEDULER=\"address_scheduler1\" # Enter the address of scheduler1\n",
    "    else:\n",
    "        SCHEDULER=\"address_scheduler2\" # Enter the address of scheduler2\n",
    "        cluster = LocalCluster(n_workers=CPU,\n",
    "                             threads_per_worker=Threads,\n",
    "                             memory_limit='400GB',\n",
    "                             dashboard_address='address_localhost') # Enter the address of local host\n",
    "        client = Client(cluster)\n",
    "        print(cluster.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare input and output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #pyScenic Databases\n",
    "    DATABASES_GLOB = os.path.join(DATABASE_FOLDER, \"mm10*.feather\")\n",
    "    \n",
    "    #pyScenic Resources\n",
    "    MOTIF_ANNOTATIONS_FNAME = os.path.join(RESOURCES_FOLDER, \"motifs-v9-nr.mgi-m0.001-o0.0.tbl\")\n",
    "    MM_TFS_FNAME = os.path.join(RESOURCES_FOLDER, 'mm_tfs.txt')\n",
    "    \n",
    "    #Input file\n",
    "    SC_EXP_FNAME = os.path.join(INPUT_FOLDER, filename)\n",
    "    \n",
    "    #Output file\n",
    "    ADJACENCIES_FNAME = os.path.join(DATA_FOLDER, \"adjacencies.tsv\")\n",
    "    MODULES_FNAME = os.path.join(DATA_FOLDER, \"modules.p\")\n",
    "    MOTIFS_FNAME = os.path.join(DATA_FOLDER, \"motifs.csv\")\n",
    "    REGULONS_FNAME = os.path.join(DATA_FOLDER, \"regulons.p\")\n",
    "    REGULONS_CSV_FNAME = os.path.join(DATA_FOLDER,\"10XHip2021_GRN.matrix.csv\") # Choose name of main output\n",
    "    META_CSV_FNAME = os.path.join(DATA_FOLDER,\"Metadata.csv\")\n",
    "    HEATMAP_FNAME = os.path.join(DATA_FOLDER, \"heatmap.png\")\n",
    "    \n",
    "    N_SAMPLES = 500\n",
    "    \n",
    "    #Make output dir\n",
    "    Path(DATA_FOLDER).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get the expression matrix (loom object), the TF list and promoter databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Get expression matrix as pd.DataFrame\n",
    "    ds = ad.read_loom(SC_EXP_FNAME,validate=False)\n",
    "    ds.X = ds.layers['raw']\n",
    "    ds.var.index = np.array(ds.var.gene_names,dtype=str)\n",
    "    ds.obs.index = np.array(ds.obs.cell_names,dtype=str)\n",
    "    pd.DataFrame(ds.obs).to_csv(META_CSV_FNAME)\n",
    "    ExprMat = pd.DataFrame(ds.to_df())\n",
    "        \n",
    "    # Get  TF list\n",
    "    tf_names = load_tf_names(MM_TFS_FNAME)\n",
    "    \n",
    "    # Get promoter region databases\n",
    "    db_fnames = glob.glob(DATABASES_GLOB)\n",
    "    def name(fname):\n",
    "        return os.path.splitext(os.path.basename(fname))[0]\n",
    "    dbs = [RankingDatabase(fname=fname, name=name(fname)) for fname in db_fnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Inference of coexpression modules (GRNBoost2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "if __name__ == '__main__':\n",
    "    ###Inference of coexpression modules, and save results\n",
    "    adjacencies = grnboost2(expression_data=ExprMat, tf_names=tf_names, verbose=True)\n",
    "    adjacencies.to_csv(ADJACENCIES_FNAME, index=False, sep='\\t')\n",
    "    \n",
    "    ###Close clients\n",
    "    client.close()\n",
    "    cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Derive potential regulons from these co-expression modules (cisTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regulons are derived from adjacencies based on three methods.\n",
    "##### The first method to create the TF-modules is to select the best targets for each transcription factor:\n",
    "- Targets with importance > the 50th percentile.\n",
    "- Targets with importance > the 75th percentile\n",
    "- Targets with importance > the 90th percentile.\n",
    "\n",
    "##### The second method is to select the top targets for a given TF: Top 50 targets (targets with highest weight)\n",
    "\n",
    "##### The alternative way to create the TF-modules is to select the best regulators for each gene. \n",
    "##### Then, these targets can be assigned back to each TF to form the TF-modules. In this way we will create three more gene-sets:\n",
    "- Targets for which the TF is within its top 5 regulators\n",
    "- Targets for which the TF is within its top 10 regulators\n",
    "- Targets for which the TF is within its top 50 regulators\n",
    "\n",
    "##### A distinction is made between modules which contain targets that are being activated and genes that are being repressed. Relationship between TF and its target, i.e. activator or repressor, is derived using the original expression profiles. The Pearson product-moment correlation coefficient is used to derive this information. In addition, the transcription factor is added to the module and modules that have less than 20 genes are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    modules = list(modules_from_adjacencies(adjacencies, ExprMat))    \n",
    "    with open(MODULES_FNAME, 'wb') as f:\n",
    "        pickle.dump(modules, f)\n",
    "    #with open(MODULES_FNAME,'rb') as f:\n",
    "    #    modules = pickle.load(f)\n",
    "    \n",
    "    ###Prune modules for targets with cis-Regulatory footprints\n",
    "    df = prune2df(dbs, modules, MOTIF_ANNOTATIONS_FNAME, num_workers=CPU)\n",
    "    df.to_csv(MOTIFS_FNAME)\n",
    "    \n",
    "    ###Pruned module dataframe can be converted to regulons\n",
    "    regulons = df2regulons(df)\n",
    "    with open(REGULONS_FNAME, 'wb') as f:\n",
    "        pickle.dump(regulons, f)\n",
    "    #with open(REGULONS_FNAME, 'rb') as f:\n",
    "    #    regulons = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Characterize cells based on regulon enrichment (AUCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    auc_mtx = aucell(ExprMat, regulons, num_workers=CPU)\n",
    "    auc_mtx.to_csv(REGULONS_CSV_FNAME)\n",
    "    sns.clustermap(auc_mtx, figsize=(12,12)).savefig(HEATMAP_FNAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
